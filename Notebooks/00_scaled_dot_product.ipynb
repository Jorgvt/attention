{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot Product Attention\n",
    "\n",
    "> Exploration of the scaled dot product attention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of the scaled dot product attention is to take a set of queries ($Q$) and compare them with a set of keys ($K$) by performing the *Dot product* between them. The concept is like taking a query, $q_i$, and comparing it with all the keys, $k_j$, to find the most similar key. This could be written as follows:\n",
    "$$\n",
    "A_i = \\sum_j q_i k_j\n",
    "$$\n",
    "\n",
    "where $A_i$ would be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a sample $q_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_i = np.linspace(1,6, num=6)\n",
    "q_i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a set of keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = np.array([1,0,-1,2,7,4])\n",
    "k_2 = np.array([1,2,3,4,7,6])\n",
    "\n",
    "k_1, k_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate how close is $q_i$ to $k_1$ and $k_2$. We would expect it to be much closer to $k_2$ than to $k_1$:\n",
    "\n",
    "> We're going to use the loop definition of the dot product to be more consistent with the math and then we'll swap to vector and matrices multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_prod(v1, v2): return np.sum(v1*v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prod(q_i, k_1), dot_prod(q_i, k_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By putting together the whole set of keys into a matrix we can build the matrix $K$, where each row corresponds to a particular key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.concatenate([k_1[None,:], k_2[None,:]], axis=0)\n",
    "K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to calculate the similarity of $q_i$ to all the $k_j$ at the same time by employing matrix multiplications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're adding an empty dim so that numpy treats q_i as a row vector\n",
    "A_i = q_i[None,:] @ K.T\n",
    "A_i, A_i.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same operation can be expressed using Einstein summation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum(\"i,ji\", q_i, K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding to this, if we stack a set of queries into a matrix $Q$ (as we did with the keys), we can calculate the simmilarity of a set of queries with a set of keys in a single operation to obtain the attention matrix, $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_1 = np.linspace(1,6, num=6)\n",
    "q_2 = np.random.randint(-1, 7, size=6)\n",
    "Q = np.concatenate([q_1[None,:], q_2[None,:]], axis=0)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Q @ K.T\n",
    "A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this matrix, the element $A_{ij}$ represents how similar is $q_i$ to $k_j$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining this matrix, we have to weight the values, $v_i$, like $O = \\sum_j v_i A_{ij}$. Before doing so, to be able to interpretate the rows of the matrix $A$ as weights, we'll take the softmax row-wise. When our vectors have high dimensionality, the softmax can push the gradients to be very low. To avoid this effect, we'll divide $A$ by $\\sqrt{d_k}$ to **scale**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A / np.sqrt(6)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A / A.max(axis=1)[:,None]\n",
    "A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some value vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_1 = np.random.randint(-1, 7, size=6)\n",
    "v_2 = np.random.randint(-1, 7, size=6)\n",
    "V = np.concatenate([v_1[None,:], v_2[None,:]], axis=0)\n",
    "V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally calculate the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = A @ V\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output matrix can be understood as a new $V$ matrix whose rows are obtained as a weighted sum of the previous $v_i$, where the weight depends on how similar the query $q_i$ is to all the other keys, $k_j$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing it as a *Keras* `Layer`\n",
    "\n",
    "> Now that we kind of understand how scaled dot product attention works, we can build a *Keras* layer to introduce it in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScaledDotProductSA(layers.Layer):\n",
    "    \"\"\"Scaled dot product self-attention layer.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dk, # Dim of the queries and keys.\n",
    "                 dv, # Dim of the values.\n",
    "                 return_attn=False, # Wether to return te attention matrix or not.\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        super(ScaledDotProductSA, self).__init__(**kwargs)\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.return_attn = return_attn\n",
    "    \n",
    "    def build(self,\n",
    "              input_shape,\n",
    "              ):\n",
    "        self.Q = layers.Dense(self.dk)\n",
    "        self.K = layers.Dense(self.dk)\n",
    "        self.V = layers.Dense(self.dv)\n",
    "    \n",
    "    def call(self,\n",
    "             inputs,\n",
    "             **kwargs,\n",
    "             ):\n",
    "        ## 1. Project the input sequence into Q, K and V.\n",
    "        Q, K, V = self.Q(inputs), self.K(inputs), self.V(inputs)\n",
    "\n",
    "        ## 2. Dot product between queries and keys.\n",
    "        A = Q @ K.T\n",
    "\n",
    "        ## 3. Scale A and apply softmax row-wise.\n",
    "        A = tf.nn.softmax(A/tf.math.sqrt(self.dk), axis=-1)\n",
    "\n",
    "        ## 4. Use A to weight the values V.\n",
    "        output = A @ V\n",
    "\n",
    "        if self.return_attn: return A, output\n",
    "        else: return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da5141a55de43f9a5c077a362efe5e2ae0cb795b0fc8676e62dbd4f64287ec27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
